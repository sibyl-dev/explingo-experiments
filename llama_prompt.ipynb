{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prerequisite libraries\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import json\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import LlamaModel, LlamaForQuestionAnswering, LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = 3\n",
    "N_RESPONSES = 3\n",
    "#EXPS = [\"ames_housing_0.csv\", \"ames_housing_1.csv\", \"ames_housing_2.csv\"]\n",
    "#EXPS = [\"mushroom_0.csv\", \"mushroom_1.csv\", \"mushroom_2.csv\"]\n",
    "#EXPS = [\"cell_phone_churn_0.csv\", \"cell_phone_churn_1.csv\", \"cell_phone_churn_2.csv\"]\n",
    "EXPS = [\"ames_housing_0.csv\", ]\n",
    "JSON_NAME = \"sample.json\"\n",
    "\n",
    "exps = []\n",
    "for exp in EXPS:\n",
    "  exp_df = pd.read_csv(exp)\n",
    "  exp_df = exp_df.sort_values(by=\"Contribution\", key=abs, ascending=False)\n",
    "  exps.append(exp_df.to_dict('records'))\n",
    "\n",
    "try:\n",
    "  with open(JSON_NAME, \"r\") as fp:\n",
    "      save_json = json.load(fp)\n",
    "except FileNotFoundError:\n",
    "  save_json = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6d4579074045de9688491444d731b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = \"./llama-2-13b-chat-hf\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir)\n",
    "tokenizer_llama = LlamaTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_exp(exp, num_features=5, include_average=True):\n",
    "  features = []\n",
    "  if num_features is None:\n",
    "    num_features = len(exp)\n",
    "  for i in range(num_features):\n",
    "    if include_average:\n",
    "      features.append(\"({}, {}, {}, {})\".format(exp[i]['Feature Name'].strip(),\n",
    "                                                exp[i]['Feature Value'],\n",
    "                                                exp[i]['Contribution'],\n",
    "                                                exp[i]['Average/Mode']))\n",
    "    else:\n",
    "      features.append(\"({}, {}, {})\".format(exp[i]['Feature Name'].strip(),\n",
    "                                                exp[i]['Feature Value'].strip(),\n",
    "                                                exp[i]['Contribution']))\n",
    "  return \", \".join(features)\n",
    "\n",
    "def show_responses(response, filename=None):\n",
    "  f = None\n",
    "  if filename is not None:\n",
    "    f = open(filename, \"w\")\n",
    "  for choice in response.choices:\n",
    "    if f is not None:\n",
    "      f.write(choice.message.content)\n",
    "      f.write(\"\\n\")\n",
    "    print(choice.message.content)\n",
    "    print(\"\\n\")\n",
    "  if f is not None:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  (\"You are a helpful assistant. \"\n",
    "           \"You are helping users understand an ML model's predictions. \"\n",
    "           \"Do not use more tokens that necessary but make your answers sound natural.\"\n",
    "           )\n",
    "question =  (\"Convert this feature contibution explanation, generated using SHAP, into a simple narrative. \"\n",
    "             \"The explanation is presented in (feature, feature_value, contribution, average_feature_value) format: \")\n",
    "explanation = parse_exp(exp, num_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_llama = transformers.pipeline(\n",
    "\"text-generation\",\n",
    "model=model,\n",
    "tokenizer=tokenizer_llama)\n",
    "# torch_dtype=torch.float16,\n",
    "# device_map=\"auto\",\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_llama(question, explanation, pipeline=pipeline_llama, tokenizer=tokenizer_llama, json=None):\n",
    "\n",
    "    input = f\"\"\"\n",
    "        <<SYS>>\n",
    "        {prompt}\n",
    "        <</SYS>>\n",
    "        [INST]\n",
    "        User:{question}\n",
    "        [/INST]\\\n",
    "        [INST]\n",
    "        User:{explanation}\n",
    "        [/INST]\\n\n",
    "\n",
    "        Assistant:\n",
    "    \"\"\"\n",
    "    \n",
    "    sequences = pipeline(\n",
    "        input,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        num_return_sequences=3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=500,\n",
    "        return_full_text=False,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    for seq in sequences:\n",
    "      generated_text = seq['generated_text']\n",
    "      # Find the start of the assistant's answer and return only that part\n",
    "      answer_start = generated_text.find(\"Assistant:\") + len(\"Assistant:\")\n",
    "      response = generated_text[answer_start:].strip()\n",
    "    \n",
    "      if json is not None:\n",
    "          if prompt not in json:\n",
    "              json[prompt] = {}\n",
    "          json[prompt][question] = response\n",
    "          #pp_result(json, prompt, question)\n",
    "      else:\n",
    "          return response\n",
    "\n",
    "def get_responses(response):\n",
    "  responses = []\n",
    "  for choice in response.choices:\n",
    "    responses.append(choice.message.content)\n",
    "  return responses\n",
    "  \n",
    "def pp_result(json, prompt, question):\n",
    "  print(\"PROMPT: %s\" % prompt)\n",
    "  print(\"QUESTION: %s\" % question)\n",
    "  print(\"===\")\n",
    "  for response in json[prompt][question]:\n",
    "    print(response)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Example usage\n",
    "results = generate_answer_llama(question, explanation, json=save_json)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(JSON_NAME, \"w\") as fp:\n",
    "  json.dump(save_json, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25ef303888f4bc08ca826e622f04cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/593 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f3dfcaac9c46efa2824b6bceb81909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/512k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740fdd06861343b686640038b525b7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/330 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c11fae597447ae9a466fbfbe31e377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7d1f44043a41efbf6ccebc655856e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57617fbbf0684a159fe60a58611ae13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c13e897184466d8f75e3b4772515e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb997749cbb64ab0ae5f192b86610f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dbf721bc1943158ed72f552dea9c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480415ccaf9949db96b9d417b413bf92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "## v2 models\n",
    "#model_path = 'openlm-research/open_llama_3b_v2'\n",
    "model_path = 'openlm-research/open_llama_7b_v2'\n",
    "\n",
    "## v1 models\n",
    "# model_path = 'openlm-research/open_llama_3b'\n",
    "# model_path = 'openlm-research/open_llama_7b'\n",
    "# model_path = 'openlm-research/open_llama_13b'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Explain this feature contibution explanation into a simple natural language sentence. The explanation is presented in (feature, feature_value, contribution, average_feature_value) format: (Above grade (ground) living area square feet, 1256, -12527.462023188567, 1684.9), (Rates the overall material and finish of the house, 5, -10743.763013432692, 6.7), (Second floor square feet, 0, -10142.290455798697, 583.0)\n",
      "\n",
      "# +\n",
      "# %%capture\n",
      "# !pip install -q git+https://github.com/davidsandberg/\n"
     ]
    }
   ],
   "source": [
    "#prompt = 'Q: What is the largest animal?\\nA:'\n",
    "question =  (\"Explain this feature contibution explanation into a simple natural language sentence. \"\n",
    "             \"The explanation is presented in (feature, feature_value, contribution, average_feature_value) format: \")\n",
    "\n",
    "input_ids = tokenizer(question+explanation, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=30\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
